# TA-LLaVA

An efficient multimodal LLM with instruction-aware visual prefix. We propose a novel multimodal adapter design that aims to compress visual tokens without losing information to address user questions. This work focuses on building cheap yet general visual assistants capable of solving a wide range of vision-language tasks. 
